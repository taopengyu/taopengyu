<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="图像分类的技能包及每一项的实验验证"><meta name="keywords" content="Machine Learn"><meta name="author" content="xiaobubuya"><meta name="copyright" content="xiaobubuya"><title>图像分类的技能包及每一项的实验验证 | xiaobubuyaのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"X2MTDP5JXS","apiKey":"b740a5c5a0afcfdb46cd9dde3f3da169","indexName":"xiaobubuya","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="xiaobubuyaのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E6%8A%80%E8%83%BD%E5%8C%85%E5%8F%8A%E6%AF%8F%E4%B8%80%E9%A1%B9%E7%9A%84%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81"><span class="toc-number">1.</span> <span class="toc-text">图像分类的技能包及每一项的实验验证</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">介绍 </span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E7%BA%BF"><span class="toc-number">4.</span> <span class="toc-text">基线</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E6%8A%80%E5%B7%A7"><span class="toc-number">5.</span> <span class="toc-text">图像分类的技巧</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E7%9A%84%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="toc-number">6.</span> <span class="toc-text">高效的训练技巧</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-1-%E5%A4%A7Batch%E8%AE%AD%E7%BB%83"><span class="toc-number">7.</span> <span class="toc-text">Trick #1: 大Batch训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-2-LR-Warm-up"><span class="toc-number">8.</span> <span class="toc-text">Trick #2: LR Warm-up</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-3-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6"><span class="toc-number">9.</span> <span class="toc-text">Trick #3: 混合精度</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96"><span class="toc-number">10.</span> <span class="toc-text">训练优化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-4-%E4%BD%99%E5%BC%A6%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">11.</span> <span class="toc-text">Trick #4: 余弦学习率衰减</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-5-%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91"><span class="toc-number">12.</span> <span class="toc-text">Trick #5: 标签平滑</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-6-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-number">13.</span> <span class="toc-text">Trick #6: 知识蒸馏</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Trick-7-Mix-up-%E5%A2%9E%E5%BC%BA"><span class="toc-number">14.</span> <span class="toc-text">Trick #7: Mix-up 增强</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A6%8F%E5%88%A9-%E6%8A%80%E5%B7%A7%E7%BB%84%E5%90%88"><span class="toc-number">15.</span> <span class="toc-text">福利: 技巧组合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">16.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img02/%E5%A4%B4%E5%83%8F.jpg"></div><div class="author-info__name text-center">xiaobubuya</div><div class="author-info__description text-center">命数如织，当为磐石</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/xiaobubuya">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">69</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">28</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xiaobubuya.cn/gallery">Gallery</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xiaobubuya.cn/slides">Slides</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/jiangnan.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">xiaobubuyaのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">图像分类的技能包及每一项的实验验证</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-03-03</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.9k</span><span class="post-meta__separator">|</span><span>Reading time: 13 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="图像分类的技能包及每一项的实验验证"><a href="#图像分类的技能包及每一项的实验验证" class="headerlink" title="图像分类的技能包及每一项的实验验证"></a>图像分类的技能包及每一项的实验验证</h1><p>原创AI公园2020-11-10 09:45:51</p>
<blockquote>
<p>作者：Pavel Semkin</p>
<p>编译：ronghuaiyang</p>
</blockquote>
<p><strong>导读</strong></p>
<blockquote>
<p>通过实验验证了图像分类技能包中每种技巧是否有效。</p>
</blockquote>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍 "></a><strong>介绍 </strong></h1><p>图像分类是计算机视觉中的一个关键问题。</p>
<p>在图像分类任务中，输入是一幅图像，输出是通常描述图像内容的类标签(如“猫”、“狗”等)。</p>
<p>近十年来，神经网络在解决图像分类问题方面取得了很大进展。神经网络在分类问题上的应用始于2012年，由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton引入AlexNet。他们的模型在ImageNet挑战中达到了63.3%的第一名精度。目前，(截至2020年8月)排名第一的是名为“FixEfficientNet-l2”的网络，其成绩为88.5%。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/08befe14fcfb464ba853dcae3a075e0a?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>图1：ImageNet数据集的提升</p>
<p>人们对给图片分配标签的任务进行了详细的研究。通常，在GitHub上搜索一个模型(或自己实现它)并在数据上训练它就足够了。你得到了一个可以准确预测标签的解决方案。</p>
<p>然而，当你对结果不满意时，改进模型可能会很棘手。你可以尝试以下方法之一：</p>
<ul>
<li><strong>使用另一个模型</strong>。例如，如果你使用ResNet系列，你可以尝试使用更大的模型或切换到最近的修改，如ResNeSt。然而，这并不总是可能的，因为你可能受到资源的限制(例如，如果你的目标部署在一个像Raspberry Pi这样的小设备)，拥有数亿个参数的最先进的模型可能无法放入内存，或者推理可能太慢。此外，通常，我们使用的是预先训练好的模型来做迁移学习，你需要为你的模型找到权重，但如果你做一些自定义更新或在GitHub上找到一个模型，这可能是一个问题。这就是为什么有时你必须修复所选择的模型，并找到其他方法来提高质量。</li>
<li><strong>增加数据集大小</strong>。增加额外的样本可以提高质量。这是一个很明显的选择，它确实可以帮助模型更好地泛化，但也存在一些问题。首先，你需要标记新数据或找到标记良好的公共数据集。在分类任务中，标记通常被认为是简单的，但这在很大程度上取决于任务的细节。例如，医学图像可能很难获取，甚至更难标记。此外，还需要确保这些新数据具有类似的分布，不会干扰模型。</li>
<li><strong>微调超参数</strong>。神经网络自己会更新数百万个参数，但有几个超参数，如优化器参数，损失权重等，需要研究人员去设置。由于超参数有很多可能的组合，如果没有任何先验知识或直觉，可能很难找到最好的一个。</li>
<li><strong>使用一些“技巧”</strong>。它们是人们用来提高性能的最佳实践。这些技巧与超参数调优不同，因为你需要了解模型内部和训练过程中发生了什么。通过在训练期间更新一些参数(例如使用一个特定的学习率策略)或在模型权值初始化期间以一种特定的方式初始化，你可以使训练更加稳定并提高最后的结果。</li>
</ul>
<p>今天我们要用最后最后一条，通过使用一些技巧来提升模型性能，这些技巧都在“Bag of Tricks for Image Classification with Convolutional Neural Networks”中被测试过，这里会重新验证一遍。</p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a><strong>数据集</strong></h1><p>在我们的实验中，我们使用Food-101数据集。它可以在Kaggle上下载。Lukas Bossard、Matthieu Guillaumin和Luc Van Gool在food101 - Mining Discriminative Components with Random Forests中介绍了该数据集。</p>
<p>它包括101种食物。每个类包含1000个图像。因此，整个数据集包含101,000张图像，并被划分为train和test两个子集。train部分为每个类包含750个图像。然而，为了提高训练速度，我们将类别数量从101个减少到21个。</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/13d668846f72471ba5d668c37b22c3e8?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>图2：来自Food-101 数据集的图像样本</p>
<p>请按照指示准备数据集：</p>
<ul>
<li>从Kaggle网站下载zip-archive：<a target="_blank" rel="noopener" href="https://www.kaggle.com/dansbecker/food101/download">https://www.kaggle.com/dansbecker/food101/download</a></li>
<li>解压缩数据</li>
<li>使用split_food-101.py将Food-101分割为训练/测试文件夹。这个脚本会解析train.txt和test.txt并复制图像到相应的子文件夹。注意，我们硬编码了将要使用的类。</li>
</ul>
<h1 id="基线"><a href="#基线" class="headerlink" title="基线"></a><strong>基线</strong></h1><p>我们使用ResNet-18架构作为基线。为了提升结果，我们使用了一个预训练过的ImageNet模型，该模型使用Adam优化器和交叉熵损失函数。默认LR为1e-4，在epochs 15和30之后，每次乘以0.1。总的来说，模型在1个Nvidia 1080Ti GPU上训练了40个epoch，batch size大小为32。我们使用PyTorch-Lightning框架来组织我们的代码。</p>
<p><strong>注</strong>：为了使我们的结果更可靠，我们在每个实验中使用不同的种子启动3次，并提供平均结果。</p>
<p>由于我们的数据集很大而且很多样，所以我们使用一个简单的增强策略。在训练期间，我们使用：</p>
<ul>
<li>RandomResizedCrop</li>
<li>HorizontalFlip</li>
<li>Normalization</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def get_training_augmentation():</span><br><span class="line">    augmentations_train = A.Compose(</span><br><span class="line">        [</span><br><span class="line">            A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),</span><br><span class="line">            A.HorizontalFlip(),</span><br><span class="line">            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),</span><br><span class="line">            ToTensorV2(),</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    return lambda img: augmentations_train(image=np.array(img))</span><br></pre></td></tr></table></figure>
<p>在验证过程中，我们遵循作者的策略，将图像的短边调整为256，保持宽高比不变。然后使用中心裁剪，得到224×224的方形区域：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def get_test_augmentation():</span><br><span class="line">    augmentations_val = A.Compose(</span><br><span class="line">        [</span><br><span class="line">            A.SmallestMaxSize(256),</span><br><span class="line">            A.CenterCrop(224, 224),</span><br><span class="line">            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),</span><br><span class="line">            ToTensorV2(),</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    return lambda img: augmentations_val(image=np.array(img))</span><br></pre></td></tr></table></figure>
<h1 id="图像分类的技巧"><a href="#图像分类的技巧" class="headerlink" title="图像分类的技巧"></a><strong>图像分类的技巧</strong></h1><p>首先，让我们把技巧分成两类：</p>
<ol>
<li><strong>高效的训练技巧</strong> — 硬件和模型相结合的技巧，可能提高性能</li>
<li><strong>训练优化</strong> — 进一步提高质量的几个有趣的方法。</li>
</ol>
<p>让我们详细讨论每一个技巧。</p>
<h1 id="高效的训练技巧"><a href="#高效的训练技巧" class="headerlink" title="高效的训练技巧"></a><strong>高效的训练技巧</strong></h1><h1 id="Trick-1-大Batch训练"><a href="#Trick-1-大Batch训练" class="headerlink" title="Trick #1: 大Batch训练"></a><strong>Trick #1: 大Batch训练</strong></h1><p>Batch大小是一个至关重要的训练参数，尽管Batch越大收敛速度越快，效果越好，但对于其最优值却不一定。这是一个有争议的，同时也是一个被广泛研究的话题。下面是一些处理这个问题的启发式方法。由于资源有限，我们试验了batch大小16、32、64、96。</p>
<p>当我们增加batch大小时，我们不改变随机梯度的期望，但减少噪音，因此，减少了方差。这意味着，batch越大，我们的学习效率就越高。一种流行的方法是在训练过程中线性缩放学习率。例如，假设我们选择1e-4作为batch大小为32的初始学习率。然后，通过改变batch大小，我们增加学习率为1e-4*b/32。然而，在我们的案例中，我们发现，Adam优化器1e-4的收敛性和稳定性更好，所以我们没有进行太多的线性缩放实验。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/c7f2b703b53e4b61bcee7da4f00a9c83?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表1：使用不同的batch size训练的结果</p>
<p>batch越大，训练时间越短，训练精度越低。</p>
<h1 id="Trick-2-LR-Warm-up"><a href="#Trick-2-LR-Warm-up" class="headerlink" title="Trick #2: LR Warm-up"></a><strong>Trick #2: LR Warm-up</strong></h1><p>遵循这一启发式，我们使用最初的几个epochs来“热身”学习率。在训练开始时(当所有的参数都远离最优参数时)使用较高的学习率可能会导致数值不稳定性的而导致质量下降。假设我们想要在前m个epochs上热身，使用初始学习率，在第i个epoch中，1≤i≤m，学习率为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def optimizer_step(self, epoch, batch_idx, optimizer, *args, **kwargs):</span><br><span class="line">    # Learning Rate warm-up</span><br><span class="line">    if self.args.warmup != -1 and epoch &lt; self.args.warmup:</span><br><span class="line">        lr = self.args.lr * (epoch + 1) / self.args.warmup</span><br><span class="line">        for pg in optimizer.param_groups:</span><br><span class="line">            pg[&quot;lr&quot;] = lr</span><br></pre></td></tr></table></figure>
<p>在那之后，我们可以使用任何策略(multi-step衰减，plateau衰减)。在我们的实验中，我们使用6 epochs 热身，直到学习率变为1e-4，然后在15和30个epochs上衰减到得1e-5，1e-6。</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/36d0544b28ab40538997905b653772d4?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>图3：学习率策略</p>
<p>总的来说，这个技巧提高了0.08%的准确率，不是很显著。</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/f2038ba88f8f48b39a943ce32478d1c9?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表2：学习率热身的结果</p>
<h1 id="Trick-3-混合精度"><a href="#Trick-3-混合精度" class="headerlink" title="Trick #3: 混合精度"></a>Trick #3: 混合精度</h1><p>在常用框架(PyTorch、TensorFlow)中，我们用32位浮点精度格式(FP32)训练我们的模型。换句话说，所有的参数，梯度，算术运算的结果都以这种格式存储。然而，由于优化的逻辑单元，现代硬件在精度较低的数据类型上可能表现出更好的性能。文章的作者表示，他们的Nvidia V100在FP32上具有14个TFLOPS，而在FP16中具有100个TFLOPS。不幸的是，我们的GPU (Nvidia 1080Ti)FP16时速度较低，所以我们不会看到FP32和FP16性能的任何显著差异。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/e7954830e5cc41aba49207e6d94f374d?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>混合精度训练的结果</p>
<p>如你所见，FP16提高了所有batch大小(BS)设置的训练速度，而且也提高了准确度。我们使用了Nvidia apex库，其中有FP32的O0优化级别和FP16的O1优化级别。在 PyTorch-Lightning 中，可以通过在命令行参数中添加—amp_level [Opt_level]在FP32和FP16之间切换。</p>
<h1 id="训练优化"><a href="#训练优化" class="headerlink" title="训练优化"></a><strong>训练优化</strong></h1><h1 id="Trick-4-余弦学习率衰减"><a href="#Trick-4-余弦学习率衰减" class="headerlink" title="Trick #4: 余弦学习率衰减"></a><strong>Trick #4: 余弦学习率衰减</strong></h1><p>除了多步衰减学习率策略外，还有一些我们可以使用的策略。例如，我们可以应用一个余弦函数来将学习率从初始值降低到0。假设有T个epoch(忽略热身阶段)，初始学习率为l，那么在epoch T时，学习率l~T~的计算为：</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/56ed812fd6014459b21c39d4f0bb7d46?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>这样做的目的是为了平稳地降低学习率，与步进衰减策略相比，可以获得更好的训练效果。在余弦衰减过程中，我们在开始和结束时慢慢降低学习速率，而在中间，下降速率几乎是线性的。</p>
<p><img src="https://p1-tt.byteimg.com/origin/pgc-image/08afa9cd1f2d46c0924408c104e99c89?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>图4：余弦学习率衰减</p>
<p>可以注意到，在我们的案例中，这种方法提高了准确率。此外，使用余弦策略的实验时间更短。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/f1258ff4bdb445968efe41365c86acc3?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表4：使用余弦学习率策略的结果</p>
<h1 id="Trick-5-标签平滑"><a href="#Trick-5-标签平滑" class="headerlink" title="Trick #5: 标签平滑"></a>Trick #5: 标签平滑</h1><p>在图像分类中，我们通常使用交叉熵损失函数：</p>
<p><img src="https://p1-tt.byteimg.com/origin/pgc-image/3955ed79a6354a98a9085b416846c00f?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>通过标签平滑，我们将二元指标yi替换为：</p>
<p><img src="https://p1-tt.byteimg.com/origin/pgc-image/a99b30f591a748de86f40768d0d8c9bc?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Based on https://github.com/pytorch/pytorch/issues/7455</span><br><span class="line">class LabelSmoothingLoss(nn.Module):</span><br><span class="line">    def __init__(self, n_classes, smoothing=0.0, dim=-1):</span><br><span class="line">        super(LabelSmoothingLoss, self).__init__()</span><br><span class="line">        self.confidence = 1.0 - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.cls = n_classes</span><br><span class="line">        self.dim = dim</span><br><span class="line"></span><br><span class="line">    def forward(self, output, target, *args):</span><br><span class="line">        output = output.log_softmax(dim=self.dim)</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            # Create matrix with shapes batch_size x n_classes</span><br><span class="line">            true_dist = torch.zeros_like(output)</span><br><span class="line">            # Initialize all elements with epsilon / N - 1</span><br><span class="line">            true_dist.fill_(self.smoothing / (self.cls - 1))</span><br><span class="line">            # Fill correct class for each sample in the batch with 1 - epsilon</span><br><span class="line">            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)</span><br><span class="line">        return torch.mean(torch.sum(-true_dist * output, dim=self.dim))</span><br></pre></td></tr></table></figure>
<p>对于独热编码，模型通常对其预测过于自信，因为这种方法迫使模型做出最大可能的logit差距。这意味着正确的类的logit和其他类别的logit之间的训练结果会有巨大的差异，同时也可能导致错误的类的logit彼此之间有很大的差异。</p>
<p>标签平滑的使用鼓励模型从全连接层产生有限的输出，这可能导致更好的泛化。它迫使模型将正确类的logit与其他类的logit之间的差异设置为依赖于ε的常数。</p>
<p><img src="https://p1-tt.byteimg.com/origin/pgc-image/9df6436ffc22427587d3f93c7a3204a7?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表5：使用标签平滑训练的结果</p>
<p>总的来说，标签平滑使我们的结果提高了0.9%，我们还减少了6分钟的训练时间。</p>
<h1 id="Trick-6-知识蒸馏"><a href="#Trick-6-知识蒸馏" class="headerlink" title="Trick #6: 知识蒸馏"></a><strong>Trick #6: 知识蒸馏</strong></h1><p>知识蒸馏，就是先训练一个复杂而重的模型(我们使用ResNet-50)，即教师模型，然后在教师的帮助下训练一个较轻的模型(学生模型)。我们假设一个更复杂的模型应该具有更高的准确率，因此，理论上，它可以提高学生模型的结果，同时保持其简单性。学生试图复制老师的结果。</p>
<p>为了进行蒸馏，我们修改了损失函数。我们根据老师和学生的得分的差别来进行惩罚。我们的损失函数从交叉熵损失，变成：</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/058ab9ee812a4e11aa8976285a14a529?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Based on https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py</span><br><span class="line">class KnowledgeDistillationLoss(nn.Module):</span><br><span class="line">    def __init__(self, alpha, T, criterion=nn.CrossEntropyLoss()):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.KLDivLoss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">    def forward(self, input, target, teacher_target):</span><br><span class="line">        loss = self.KLDivLoss(</span><br><span class="line">            F.log_softmax(input / self.T, dim=1),</span><br><span class="line">            F.softmax(teacher_target / self.T, dim=1),</span><br><span class="line">        ) * (self.alpha * self.T * self.T) + self.criterion(input, target) * (</span><br><span class="line">            1.0 - self.alpha</span><br><span class="line">        )</span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure>
<p>我们使用ResNet-50作为教师模型。对模型进行了标签平滑、余弦退火LR和线性预热的训练，获得了92.18%的Top-1准确率。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/c858082b4508413baf7faa88ecc4a369?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表6：使用知识蒸馏的训练结果</p>
<p>我们在准确率上取得了显著的增长，但增加了训练时间，因为我们需要从老师那里得到预测。</p>
<h1 id="Trick-7-Mix-up-增强"><a href="#Trick-7-Mix-up-增强" class="headerlink" title="Trick #7: Mix-up 增强"></a><strong>Trick #7: Mix-up 增强</strong></h1><p>Mix-up是一种增强技术，构造一个新的图像作为两个其他的线性组合。假设我们有两个batch的样本(我们取当前的batch和早期迭代中的batch)，我们所做的是随机洗牌第二个batch，并从这两个batch中创建一个线性组合图像：</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/382ab9dac3504b42b9ff2a477863adae?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>作为目标，我们从这两个batch中取标签。我们计算每个标签的损失，并返还加权总和作为总损失：</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/3ba679271afd445fad1a87fdbf3318ac?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>λ是一个来自β分布的随机数。</p>
<p>此外，还可以为这个新样本创建一个增强目标，作为原始目标的线性组合(如果目标是one-ho编码或平滑的)。</p>
<p>这一技巧有助于减少高置信度预测的数量，并可以提高准确率，但对人类来说，可能很难判断增强的图片是什么。</p>
<p><img src="https://p6-tt.byteimg.com/origin/pgc-image/562875a14a8541f08dd166d18f002a63?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>图5：mix-up增强的例子</p>
<p>代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">    def mixup_batch(self, x, y, x_previous, y_previous):</span><br><span class="line">        lmbd = (</span><br><span class="line">            np.random.beta(self.args.mixup_alpha, self.args.mixup_alpha)</span><br><span class="line">            if self.args.mixup_alpha &gt; 0</span><br><span class="line">            else 1</span><br><span class="line">        )</span><br><span class="line">        if x_previous is None:</span><br><span class="line">            x_previous = torch.empty_like(x).copy_(x)</span><br><span class="line">            y_previous = torch.empty_like(y).copy_(y)</span><br><span class="line">        batch_size = x.size(0)</span><br><span class="line">        index = torch.randperm(batch_size)</span><br><span class="line">        # If current batch size != previous batch size, we take only a part of the previous batch</span><br><span class="line">        x_previous = x_previous[:batch_size, ...]</span><br><span class="line">        y_previous = y_previous[:batch_size, ...]</span><br><span class="line">        x_mixed = lmbd * x + (1 - lmbd) * x_previous[index, ...]</span><br><span class="line">        y_a, y_b = y, y_previous[index]</span><br><span class="line">        return x_mixed, y_a, y_b, lmbd</span><br><span class="line"></span><br><span class="line">class MixUpAugmentationLoss(nn.Module):</span><br><span class="line">    def __init__(self, criterion):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    def forward(self, input, target, *args):</span><br><span class="line">        # Validation step</span><br><span class="line">        if isinstance(target, torch.Tensor):</span><br><span class="line">            return self.criterion(input, target, *args)</span><br><span class="line">        target_a, target_b, lmbd = target</span><br><span class="line">        return lmbd * self.criterion(input, target_a, *args) + (</span><br><span class="line">            1 - lmbd</span><br><span class="line">        ) * self.criterion(input, target_b, *args)</span><br></pre></td></tr></table></figure>
<p>应用该技术的结果如下表所示：</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/b608fba5a3364fa39d1866631d8d3c63?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表7：使用mix-up增强的训练结果</p>
<p>通过平滑标签来进行两个batch之间的Mix-up增强，可以提高准确率，但需要更多的时间。</p>
<h1 id="福利-技巧组合"><a href="#福利-技巧组合" class="headerlink" title="福利: 技巧组合"></a><strong>福利: 技巧组合</strong></h1><p>最后，我们将这些技巧结合在一起，重新进行了实验。总体来说，我们使用了：</p>
<ul>
<li>线性学习率热身</li>
<li>余弦学习率策略</li>
<li>标签平滑</li>
<li>知识蒸馏</li>
</ul>
<p>可以预料，这些技巧的组合会给我们带来强大的改进，因为我们结合了最好的技巧。这种设置可以得到性能的提升。总的来说，我们将基线准确率提高了1%。你可以看到汇总表如下：</p>
<p><img src="https://p3-tt.byteimg.com/origin/pgc-image/9c31afcabf354d129b688c3b45d5aa22?from=pc" alt="图像分类的技能包及每一项的实验验证"></p>
<p>表8：结果汇总</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如上所示，以不同的方式改变训练过程可以帮助你提高准确率，但它是依赖于任务和数据的。这就是为什么，在我们的案例中，改进并不是那么显著，因为基线模型已经能够达到很高的结果了。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">xiaobubuya</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xiaobubuya.github.io/图像分类的技能包及每一项的实验验证/">https://xiaobubuya.github.io/图像分类的技能包及每一项的实验验证/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learn/">Machine Learn</a></div><div class="social-share pull-right" data-disabled="facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/22%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"><i class="fa fa-chevron-left">  </i><span>22个神经网络结构设计/可视化工具</span></a></div><div class="next-post pull-right"><a href="/linux%E8%BF%9B%E7%A8%8B/"><span>linux进程</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/jiangnan.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By xiaobubuya</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>